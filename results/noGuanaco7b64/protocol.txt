local_rank: -1
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 4
learning_rate: 0.0002
max_grad_norm: 0.3
weight_decay: 0.001
lora_alpha: 64
lora_dropout: 0.1
lora_r: 256
max_seq_length: 4096
model_name: meta-llama/Llama-2-7b-hf
use_4bit: True
use_8bit: False
use_nested_quant: False
bnb_4bit_compute_dtype: float16
bnb_4bit_quant_type: nf4
num_train_epochs: 6
fp16: False
bf16: True
packing: False
gradient_checkpointing: True
optim: paged_adamw_32bit
lr_scheduler_type: constant
max_steps: 10000
warmup_ratio: 0.01
group_by_length: True
save_steps: 1000
logging_steps: 10
merge_and_push: True
output_dir: ./results/noGuanaco7b64
train_eval_dir: ./data/train_test_datasets/run_5_noGuanaco
wnb_project: LLama
Dataset: train redewiedergabe: Dataset({
    features: ['text'],
    num_rows: 665
})
Dataset: train arguments: Dataset({
    features: ['text'],
    num_rows: 484
})
Dataset: train bsp_ds: Dataset({
    features: ['text'],
    num_rows: 506
})
Dataset: train synthetic_processed: Dataset({
    features: ['text'],
    num_rows: 358
})
Dataset: train train_all: Dataset({
    features: ['text'],
    num_rows: 6700
})
Dataset: eval redewiedergabe: Dataset({
    features: ['text'],
    num_rows: 74
})
Dataset: eval arguments: Dataset({
    features: ['text'],
    num_rows: 54
})
Dataset: eval bsp_ds: Dataset({
    features: ['text'],
    num_rows: 57
})
Dataset: eval synthetic_processed: Dataset({
    features: ['text'],
    num_rows: 40
})
