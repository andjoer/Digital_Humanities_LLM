local_rank: -1
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 4
learning_rate: 0.0002
max_grad_norm: 0.3
weight_decay: 0.001
lora_alpha: 64
lora_dropout: 0.1
lora_r: 512
max_seq_length: 4096
model_name: meta-llama/Llama-2-7b-hf
use_4bit: True
use_8bit: False
use_lora: True
use_nested_quant: False
bnb_4bit_compute_dtype: float16
bnb_4bit_quant_type: nf4
num_train_epochs: 8
fp16: False
bf16: True
packing: False
gradient_checkpointing: True
optim: paged_adamw_32bit
lr_scheduler_type: constant
max_steps: 10000
warmup_ratio: 0.01
group_by_length: True
save_steps: 500
logging_steps: 10
merge_and_push: True
output_dir: ./results/DettmersAll7b64_test
train_eval_dir: ./data/train_test_datasets/run_4_onlybsp
wnb_project: LLama
eval_collator: completion
train_collator: completion
eval_steps: 100
Dataset: train bsp_ds: Dataset({
    features: ['text'],
    num_rows: 506
})
Dataset: eval bsp_ds: Dataset({
    features: ['text'],
    num_rows: 57
})
